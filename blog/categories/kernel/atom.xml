<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel | iCen DiaMondinG]]></title>
  <link href="http://joycn.github.com/blog/categories/kernel/atom.xml" rel="self"/>
  <link href="http://joycn.github.com/"/>
  <updated>2013-06-26T00:01:22+08:00</updated>
  <id>http://joycn.github.com/</id>
  <author>
    <name><![CDATA[joycn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[让python webserver 能好好利用多核]]></title>
    <link href="http://joycn.github.com/blog/2013/06/11/python-multiple-core/"/>
    <updated>2013-06-11T22:25:00+08:00</updated>
    <id>http://joycn.github.com/blog/2013/06/11/python-multiple-core</id>
    <content type="html"><![CDATA[<p>之前都是搞内核方面的东西,最近有个项目,要做管理端,正好,想趁机去学习下web的东西.业务方要求10Kqps.对这个没概念,又不想堆机器.就先调研了下python的框架.之前一直很喜欢web.py,因为轻量级的框架,但是传说性能很差,要靠fastcgi还提高性能.同事又推荐了tornado,大概看了下介绍,和web.py很像,很轻.(后来才知道,跟web.py很有渊源)还是异步的,不太清怎么个异步的方式.自己简单用hello,world测了下性能,单核大概有3K+的QPS.跟网上说的性能差不太多.有时间了再测下web.py之类的.开始说正题</p>

<!-- more -->


<p>看网上有介绍说用nginx做反向代理,然后跑多核的效果.性能是提升了好几倍.但数据包在本机要走两遍协议栈,感觉性能上还是有些消耗,加上cache之类的性能损失.还没测性能,就先想着自己写个模块试下效果.其实怎么滴还是要先简单实际测下,看到底会有多大的损失.</p>

<p>模块写起来还比较简单,大概半天的时间初版搞定.因为没做优化,对性能还是比较忐忑的.结果试了下,还不错,12核的cpu,都利用起来了,差不多性能提高了10~11倍,基本符合预期.后续进行进一步的优化,使功能和性能都能进一步的提升.</p>

<p>好久都没写东西了,乱七八糟的杂事太多了.好不容易有个清净的时候,能写个代码.还有很多事情没做,打算关注下其他的方面,算是自己视野的拓展吧.一直在底层呆着,会呆的...</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kernel_bypass]]></title>
    <link href="http://joycn.github.com/blog/2013/04/12/kernel-bypass/"/>
    <updated>2013-04-12T08:47:00+08:00</updated>
    <id>http://joycn.github.com/blog/2013/04/12/kernel-bypass</id>
    <content type="html"><![CDATA[<p>现在10Gbe网卡现在已经比较通用了,目前40Gbe的网卡也已经出来一段时间了.随着网卡的不断发展,系统网络栈的设计,越来越成为数据包处理的瓶颈.这不仅有系统的问题,也跟特定的应用有关,毕竟有的应用用不到太多内核中相关的过于复杂的设计.现在kernel_bypass也变得越来越流行.主流的kernel_bypass的方案有下面这么几个:</p>

<!-- more -->


<ul>
<li><a href="http://www.ntop.org/products/pf_ring/dna/">ntop.org DNA</a>.</li>
<li><a href="http://info.iet.unipi.it/~luigi/netmap/">netmap.</a></li>
<li><a href="http://www.intel.com/content/www/us/en/intelligent-systems/intel-technology/packet-processing-is-enhanced-with-software-from-intel-dpdk.html">Intel DPDK.</a></li>
<li>Myricom <a href="https://www.myricom.com/software/sniffer10g.html">Sniffer10G</a> and <a href="https://www.myricom.com/software/dbl.html">DBL</a>.</li>
<li>SolarFlare <a href="http://www.openonload.org/">OpenOnload</a>.</li>
<li><a href="http://www.napatech.com/products/network_adapters.html">Napatech</a>.</li>
</ul>


<p>Myricom和Napatech都要用到自己特定的硬件,DNA又需要license.Napatech又需要DNA,又需要硬件.netmap是完全开源的,intel DPDK现在也开源了(网址就是www.dpdk.org).之前看过netmap的介绍,自己也简单试了下,效果没有想象的那么好,而且大部分也修改的驱动也是intel网卡的驱动.既然DPDK已经开源了,就先了解下DPDK方面的东西了.</p>

<h2>总结下DPDK是如何去实现80Mpps的目标的</h2>

<p>正常模式下存在的问题</p>

<ul>
<li>对于大量的网卡中断,系统已经跟不上中断的速度.</li>
<li>对于linux进程切换的消耗.</li>
<li>CPU的速度与PCIe速度之间的差距</li>
<li>cache与访存的问题</li>
<li>数据共享的问题</li>
<li>页表的频繁更新</li>
</ul>


<p>对于大量中断的问题,linux原本已经做了一部分优化,就是napi,按理说应该已经可以很大的减少硬件中断带来的性能损耗.但是,并没有减少硬件中断的次数.在napi的逻辑里,在网卡中断的处理逻辑是,检查该网卡是否已经有需要处理的数据包,如果有的话,不需要再做后续的处理,napi会在处理之前的数据包的时候,把现在的数据包也一起处理了,从而减少了硬中断的工作量.但是,中断的频率并没有改变.DPDK的方法其实,就是取消中断,靠系统,去主动的polling网卡,减少中断的开销.这样,虽然在网卡闲的时候,额外功会比较多,但是在网卡忙的时候,有效功就很高了.</p>

<p>对于进程切换的问题,绑定逻辑进程到固定的cpu,减少进程间的切换带来的消耗.由于处理数据包大部分的工作都交给用户态进程来做.所以,kernel<->userspace之间的切换也少了很多.而且还减少了,进程在不通cpu上切换,带来的numa相关的问题</p>

<p>对于PCIe速度的差异,以及访问内存的差异.跟第一点也有一定的关系,其实通过polling,一次取数据的时候,尽可能多的取数据,减少访存的次数.对于访问内存的差异,可以通过pre-fetch的方式预取,PCIe的话,可以直接通过DDIO将数据包放到L3cache中.</p>

<p>数据共享的问题,其实还是说,逻辑放到了用户态,用户可以自己去实现逻辑,通过合理的方式,减少为了保证数据一致性,使用同步机制带来的性能损耗</p>

<p>页表的更新..这个,不太熟,大概还是因为说,intel通过内核huge page的方式,尽可能的减少页表的更新.后面了解下这里</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux tcp三次握手建立过程]]></title>
    <link href="http://joycn.github.com/blog/2013/03/19/three-handshakes/"/>
    <updated>2013-03-19T08:40:00+08:00</updated>
    <id>http://joycn.github.com/blog/2013/03/19/three-handshakes</id>
    <content type="html"><![CDATA[<p>连接建立请求的处理步骤主要分为两步：1，首先，将请求放入listen的socket的syn 队列中，然后发送synack等待client返回ack。2，收到用户返回的ack后检查合法性后为请求建立新的socket，然后从syn队列中移除 该请求，放到accpet队列中。</p>

<!-- more -->


<p>ipv4所有tcp包都是在tcp_v4_rcv这个函数中处理的。对于请求建立连接的数据报当然也不例外。当对包进行一系列的安全检查后，会调用 __inet_lookup_skb来查找这个数据包是否是有对应的socket进行处理。首先在ehash里查找对应的socket，如果没有的话才去listening_hash里查找连接。意思是 通过四元组，首先检查是否是已建立连接的socket，如果不是的话才检查是否是服务器listen的socket。对于建立连接的请求，肯定是在 listen_hash中可以找到。（当然，不排除是已经建立了连接，但是client之前重传的syn包）。如果没有找到对应的socket，xfrm4_policy_check函数，根据返回结构确定是否向client发送rst包。我们先暂不考虑此时sock的状态已经是time_wait的情况。后续补充。然后调用tcp_v4_do_rcv进行后续的处理。</p>

<p>tcp_v4_do_rcv首先依旧是一系列的安全检查，当通过安全检查后，对于建立连接的请求，这时sock的状态应该是listen。然后调用 tcp_v4_hnd_req对数据报进行处理。首先调用inet_csk_search_req函数，看是否已经将该请求放入了syn队列中，就是本文 最开头说的第一步是否已经做了，如果做了肯定可以找到对应的request_sock，对于刚刚请求的，肯定是找不到的，然后我们继续在建立连接的 socket队列中查找，如果是已经建立了连接的（客户端重传的syn包）还要判断当前状态是否是time_wait如果是的就返回null，不是那就换回 对应的soket，如果到这时候还没找到，那么就返回原有的sock（没有找到对应的request_sock，就是没有完成本文开始说的第一步，并且没 有对这个请求建立连接）。这时返回到了tcp_v4_do_rcv函数，一般的连接请求肯定返回的还是原来的sock，所以就会进行之后的处理 tcp_rcv_state_process。这个函数主要用来处理接受方面的状态迁移，但是不包括ESTABLISHED和TIME_WAIT。对于目 前，sock的状态肯定是listen，收到syn包，那么调用icsk->icsk_af_ops->conn_request，其实对应 调用的是tcp_v4_conn_request。首先，还是要做一些检查。tcp肯定该不会处理发向多播地址或者组播地址的syn包。通过 inet_csk_reqsk_queue_is_full检查syn queue 是否已经满了(queue->listen_opt->qlen >> queue->listen_opt->max_qlen_log)，如果满的话，同时没有打开syncookie功能的话，只能drop掉该包。然后通过sk_acceptq_is_full检查接受队列是否已经满了(sk->sk_ack_backlog > sk->sk_max_ack_backlog)，并且有没有被处理过的syn（没有收到ack并且没有发生synack重传）。 如果这样的话也会drop掉这个包。通过都没问题，当然就该分配一个新的请求连接放到syn queue中去了。这过程中主要是要根据客户端发过来的数据报中的tcp options进行对新建的request_sock进行初始化。然后调用__tcp_v4_send_synack向client发送synack，并 将这个request_sock添加到syn队列中去。</p>

<p>第一步完成。当client返回ack后进行第二步：</p>

<p>前面跟第一步的步骤是一样的，分叉口是在进入tcp_v4_hnd_req函数后，当调用inet_csk_search_req查找半连接，肯定是可 以查到这个连接的。然后调用tcp_check_req对这个包进行检查，首先还是进行包结构的检查，检查seq号，如果和刚才的是已建立的半连接的是相 同的，那么重新发送调用req->rsk_ops->rtx_syn_ack其实也就是tcp_v4_send_synack向请求方发送一 个synack。如果ack_seq不是期望的值，那么返回当前sock，然后在tcp_rcv_state_process中返回1，调用 tcp_v4_send_reset进行相应的处理，如果该包设置了rst包或者是本机产生的包，那么我们不做处理。如果是其他的，那么我们就发送rst 包给请求方。如果window大小跟之前有很大不同，那么调用req->rsk_ops->send_ack也就是tcp_send_ack 只是单单返回一个ack。如果包的seq和之前的是一样的，那么清除TCP_FLAG_SYN位。然后检查syn和rst标志位，如果设置了，那么还是调 用tcp_v4_send_reset进行相应的处理。检查都通过 后，inet_csk(sk)->icsk_af_ops->syn_recv_sock来新建新的连接。并从syn 队列中删除对应的syn request。将新的连接添加到sock的accept队列中去。返回到tcp_v4_do_rcv中进入tcp_child_process函数确定 没有其他用户使用该新生成的连接后，如果有的话就将该连接加入到backlog中去。调用tcp_rcv_state_process。调用tcp_ack对该包进行检查，功能后面再了解，进行检查后修改连接的状态为TCP_ESTABLISHED后对该连接进行一些另外的设置。新建完成</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在使用pktgen如何利用网卡多队列功能]]></title>
    <link href="http://joycn.github.com/blog/2013/03/15/pktgen-multiple/"/>
    <updated>2013-03-15T22:16:00+08:00</updated>
    <id>http://joycn.github.com/blog/2013/03/15/pktgen-multiple</id>
    <content type="html"><![CDATA[<p>之前奇怪,为什么pktgen明明有QUEUE_MAP_CPU的flags,为什么使用起来还是单核的效果.今天同事说起来,说自己可以用多核.然后我也就跟着试了下.原来还真可以.只是pktgen的samples并没有提到这个(或许我看的比较老)</p>

<!-- more -->


<p>简单说下原理,涉及到网卡发送队列的一点点内容.当pktgen的脚本设置了QUEUE_MAP_CPU这个flag后,会在构造包的时候,将当前cpu的id绑定到skb的mapqueue里,这样就实现了多队列的效果.我们在insmod的时候,pktgen会根据cpu的个数来创建同等数量的线程,主要还是因为数据发送后的一些工作比较消耗CPU,这样我们绑定后,就将tx_action分配到了本地的cpu上.
原来我们可以通过dev_name@XX的形式来为通一个设备,加同一个设备添加到多个thread上去.比如,我们有一个eth0的多队列网卡,我们可以通过pgset "add_device eth0@1"的形式,将eth0添加到某个thread上去.所以我们就可以用下面的方式将同一个网卡绑定给多个thread.</p>

<p>{% codeblock %}
for ((id=0;processor&lt;$CPUS;id++))
do
PGDEV=/proc/net/pktgen/kpktgend_$id
pgset "add_device eth0@$processor"
done
{% endcodeblock %}</p>

<p>这样我们就可以利用网卡的队列,利用多核来发包了.双路4核,关闭超线程的cpu用8599发包,差不多速率在12Mpps, 去年这时候的时候跟intel的人聊过,说82599差不多利用4个队列就能完成9M pps的发包速率,今天试了下,还真是,9+Mpps的速率.</p>

<p>之前只改了自己一部分功能方面的代码,今天因为这个问题,看了下源码,里面的代码写的还是不错的.改天可以再去仔细的看下,毕竟以后估计要通过他来完成更多的工作</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kernel中关于gso gro的一些内容]]></title>
    <link href="http://joycn.github.com/blog/2013/03/14/kernel-gro-gso/"/>
    <updated>2013-03-14T22:42:00+08:00</updated>
    <id>http://joycn.github.com/blog/2013/03/14/kernel-gro-gso</id>
    <content type="html"><![CDATA[<p>之前在lvs利用网卡gro gso功能的时候遇到了一个bug,所以就想去了解下kernel关于gro gso的实现.简单介绍网上有一大堆,这里简单介绍下</p>

<!-- more -->


<h2>offload</h2>

<p>offload特性都是为了提升网络收/发性能。TSO、UFO和GSO是对应网络发送，在接收方向上对应的是LRO、GRO。</p>

<p>TSO
TSO(TCP Segmentation Offload)，是一种利用网卡对TCP数据包分片，减轻CPU负荷的一种技术，有时也被叫做 LSO (Large segment offload) ，TSO是针对TCP的，UFO是针对UDP的。如果硬件支持 TSO功能，同时也需要硬件支持的TCP校验计算和分散/聚集 (Scatter Gather) 功能。</p>

<p>GSO
GSO(Generic Segmentation Offload)，它比TSO更通用，基本思想就是尽可能的推迟数据分片直至发送到网卡驱动之前，此时会检查网卡是否支持分片功能（如TSO、UFO）, 如果支持直接发送到网卡，如果不支持就进行分片后再发往网卡。这样大数据包只需走一次协议栈，而不是被分割成几个数据包分别走，这就提高了效率。</p>

<p>LRO
LRO(Large Receive Offload)，通过将接收到的多个TCP数据聚合成一个大的数据包，然后传递给网络协议栈处理，以减少上层协议栈处理 开销，提高系统接收TCP数据包的能力。</p>

<p>GRO
GRO(Generic Receive Offload)，基本思想跟LRO类似，克服了LRO的一些缺点，更通用。后续的驱动都使用GRO的接口，而不是LRO。</p>

<h2>内核的一些实现</h2>

<p>其实其他的都还简单,参考下别人的文章也就了解了.之前一直没理解,收到的包,到底是怎么组织到一起的.组织的核心实现,是通过skb_gro_receive函数来完成的.
简单说下原理,gro接收到的所有包,都是通过skb中的frag_list来管理的.当收到第一个包的时候,对新建专门新建一个数据包,来存包第一个数据包的报头信息,然后将新的数据包中的frag_list指向刚刚收到的这个数据包,然后,在后续收到的数据包的时候,就直接将新的数据包连接到frag_list的结尾.这样就通过frag_list的方式将所有的数据包就都串到了一起.
这里就直接copy代码,然后在通过注释的方式来解释了.
{% codeblock lang:c %}
int skb_gro_receive(struct sk_buff <em>*head, struct sk_buff </em>skb)
{</p>

<pre><code>struct sk_buff *p = *head;
struct sk_buff *nskb;
struct skb_shared_info *skbinfo = skb_shinfo(skb);
struct skb_shared_info *pinfo = skb_shinfo(p);
unsigned int headroom;
unsigned int len = skb_gro_len(skb);
unsigned int offset = skb_gro_offset(skb);
unsigned int headlen = skb_headlen(skb);

if (p-&gt;len + len &gt;= 65536)
    return -E2BIG;
</code></pre>

<p>/<em>
当第一次数据包到达的时候,</em>head指向的是第一个数据包的地址,而对应的frag_list为空,当第二个数据包到的时候,也就是这里处理的第一个数据包到达的时候,frag_list肯定是空的,因为gro不会处理分片的数据包,也正因为如此,所以gro才能使用frag_list.
*/</p>

<pre><code>if (pinfo-&gt;frag_list)
    goto merge;
</code></pre>

<p>/<em>
这里的代码其实也没什么说的,就是说如果skb中headlen,小于报头的长度,就需要从page里的数据拷贝到</em>head里的page就好了
*/</p>

<pre><code>else if (headlen &lt;= offset) {
    skb_frag_t *frag;
    skb_frag_t *frag2;
    int i = skbinfo-&gt;nr_frags;
    int nr_frags = pinfo-&gt;nr_frags + i;

    offset -= headlen;

    if (nr_frags &gt; MAX_SKB_FRAGS)
        return -E2BIG;

    pinfo-&gt;nr_frags = nr_frags;
    skbinfo-&gt;nr_frags = 0;

    frag = pinfo-&gt;frags + nr_frags;
    frag2 = skbinfo-&gt;frags + i;
    do {
        *--frag = *--frag2;
    } while (--i);

    frag-&gt;page_offset += offset;
    frag-&gt;size -= offset;

    skb-&gt;truesize -= skb-&gt;data_len;
    skb-&gt;len -= skb-&gt;data_len;
    skb-&gt;data_len = 0;

    NAPI_GRO_CB(skb)-&gt;free = 1;
    goto done;
} else if (skb_gro_len(p) != pinfo-&gt;gso_size)
    return -E2BIG;
</code></pre>

<p>/<em>
这里就是当frag_list为空的时候,新建一个head头部来存放,数据包头部的信息
</em>/</p>

<pre><code>headroom = skb_headroom(p);
nskb = netdev_alloc_skb(p-&gt;dev, headroom + skb_gro_offset(p));
if (unlikely(!nskb))
    return -ENOMEM;

__copy_skb_header(nskb, p);
nskb-&gt;mac_len = p-&gt;mac_len;

skb_reserve(nskb, headroom);
__skb_put(nskb, skb_gro_offset(p));

skb_set_mac_header(nskb, skb_mac_header(p) - p-&gt;data);
skb_set_network_header(nskb, skb_network_offset(p));
skb_set_transport_header(nskb, skb_transport_offset(p));

__skb_pull(p, skb_gro_offset(p));
memcpy(skb_mac_header(nskb), skb_mac_header(p),
       p-&gt;data - skb_mac_header(p));

*NAPI_GRO_CB(nskb) = *NAPI_GRO_CB(p);
</code></pre>

<p>/<em>
如果frag_list为null,那么,我就把head放到frag_list.然后将新的指针做为head,并把head->pre记录frag_list中的最后一个,用于后续再来包的话,直接添加到pre->next就好了.这样就直接连到了frag_list的最后
</em>/</p>

<pre><code>skb_shinfo(nskb)-&gt;frag_list = p;
skb_shinfo(nskb)-&gt;gso_size = pinfo-&gt;gso_size;
pinfo-&gt;gso_size = 0;
skb_header_release(p);
nskb-&gt;prev = p;

nskb-&gt;data_len += p-&gt;len;
nskb-&gt;truesize += p-&gt;len;
nskb-&gt;len += p-&gt;len;

*head = nskb;
nskb-&gt;next = p-&gt;next;
p-&gt;next = NULL;

p = nskb;
</code></pre>

<p>merge:</p>

<pre><code>if (offset &gt; headlen) {
    unsigned int eat = offset - headlen;

    skbinfo-&gt;frags[0].page_offset += eat;
    skbinfo-&gt;frags[0].size -= eat;
    skb-&gt;data_len -= eat;
    skb-&gt;len -= eat;
    offset = headlen;
}

__skb_pull(skb, offset);

p-&gt;prev-&gt;next = skb;
p-&gt;prev = skb;
skb_header_release(skb);
</code></pre>

<p>done:</p>

<pre><code>NAPI_GRO_CB(p)-&gt;count++;
p-&gt;data_len += len;
p-&gt;truesize += len;
p-&gt;len += len;

NAPI_GRO_CB(skb)-&gt;same_flow = 1;
return 0;
</code></pre>

<p>}
{% endcodeblock %}</p>

<p>其他地方的逻辑,代码很容易看明白的.</p>

<p>不过问题似乎不是出在这里,后面在把gso的内容给补充下</p>

<p>临了了抱怨句,白天看代码效率真低,半天没能去静下心来看这里的代码,晚上看了一会就明白里面的意思了.</p>
]]></content>
  </entry>
  
</feed>
